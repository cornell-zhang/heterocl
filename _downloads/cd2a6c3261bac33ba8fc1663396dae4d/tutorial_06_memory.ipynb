{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Memory Customization\n\n**Author**: Yi-Hsiang Lai (seanlatias@github)\n\nIn this tutorial, we demonstrate how memory customization works in HeteroCL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import heterocl as hcl\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Customization in HeteroCL\nThere are two types of memory customization in HeteroCL. The first one is\nsimilar to what we have seen in\n`sphx_glr_gallery_tutorial_04_compute.py`, where we demonstrate some\nprimitives that will be synthesized as pragmas. An example of such primitive\nis ``partition``. Following is an example. Note that the primitive is\ndirectly applied on the schedule instead of a stage. This is because we are\nmodifying the property of a tensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hcl.init()\n\nA = hcl.placeholder((10, 10), \"A\")\n\n\ndef kernel(A):\n    return hcl.compute(A.shape, lambda x, y: A[x][y] + 1, \"B\")\n\n\ns = hcl.create_schedule(A, kernel)\ns.partition(A)\nprint(hcl.lower(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the IR, we should see a line that annotates tensor ``A`` to be\npartitioned completely.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>For more information, please see\n   :obj:`heterocl.schedule.Schedule.partition`</p></div>\n\nAnother example is to reshape a tensor. This is helpful when we combine\npartitioning with loop titling. In this example, we split the inner axis\n``y`` and also reshape the output tensor ``B``. After that, we pipeline\nthe middle axis ``yo`` and partition the output tensor accordingly. **Note\nthat the** ``reshape`` **primitive cannot be applied to the input tensors.**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hcl.init()\n\ns = hcl.create_schedule(A, kernel)\nyo, yi = s[kernel.B].split(kernel.B.axis[1], 5)\ns[kernel.B].pipeline(yo)\ns.reshape(kernel.B, (10, 2, 5))\ns.partition(kernel.B, dim=3)\nprint(hcl.build(s, target=\"vhls\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Reuse in HeteroCL\nThe other type of memory customization primitives involves the introduction\nof allocation of new memory buffers. An example is data reuse. The idea of\ndata reuse is to reduce the number of accesses to a tensor by introducing\nan intermediate buffer that holds the values being reused across different\niterations. This finally leads to better performance in hardware.\n\n## Example: 2D Convolution\nTo demonstrate this, we use the computation of 2D convolution as an example.\nLet's see how we can define 2D convolution in HeteroCL.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hcl.init()\n\nA = hcl.placeholder((6, 6), \"A\")\nF = hcl.placeholder((3, 3), \"F\")\n\n\ndef kernel(A, F):\n    r = hcl.reduce_axis(0, 3)\n    c = hcl.reduce_axis(0, 3)\n    return hcl.compute(\n        (4, 4), lambda y, x: hcl.sum(A[y + r, x + c] * F[r, c], axis=[r, c]), \"B\"\n    )\n\n\ns = hcl.create_schedule([A, F], kernel)\nprint(hcl.lower(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above example, we convolve the input tensor ``A`` with a filter ``F``.\nThen, we store the output in tensor ``B``. Note that the output shape is\ndifferent from the shape of the input tensor. Let's give some real inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hcl_A = hcl.asarray(np.random.randint(0, 10, A.shape))\nhcl_F = hcl.asarray(np.random.randint(0, 10, F.shape))\nhcl_B = hcl.asarray(np.zeros((4, 4)))\nf = hcl.build(s)\nf(hcl_A, hcl_F, hcl_B)\nprint(\"Input:\")\nprint(hcl_A)\nprint(\"Filter:\")\nprint(hcl_F)\nprint(\"Output:\")\nprint(hcl_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To analyze the data reuse, let's take a closer look to the generated IR.\nTo begin with, we can see that in two consecutive iterations of ``x`` (i.e.,\nthe inner loop), there are 6 pixels that are overlapped, as illustrated in\nthe figure below. Without any optimization, **we are reading 9 values from\nthe input for each iteration**.\n\n.. figure:: ../../../tutorials/moving_x/Slide1.png\n   :scale: 60 %\n\n### Introduce Data Reuse: Window Buffer\nTo reuse the overlapped pixels, we can introduce a reuse buffer. Since the\nfilter moves like a window, we call the buffer a window buffer ``WB``. The\nwindow buffers stores the reused pixels and also the new pixels that will\nbe used in the current iteration. For each iteration, to update the values\ninside the window buffer, the last two columns, in this case, shift left.\nAfter that, the last column is replaced with the pixels read from the\ninput. Now, we only **read 3 values from the input for each iteration**.\n\n.. figure:: ../../../tutorials/moving_x/Slide2.png\n   :scale: 60 %\n\nTo introduce such reuse buffers in HeteroCL, we use the API ``reuse_at``.\nThe first argument is the **tensor** whose values will be reused. The\nsecond argument is the output **stage** that reuses the values of the\ntensor. The reason why we need to specify this is because we may have\nmultiple stages reusing the values from the same input tensor. The third\nargument is the desired axis to be reused. It must be the output axis.\nFinally, we can specify the name of the reuse buffer. The API returns\na new tensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s_x = hcl.create_schedule([A, F], kernel)\nWB = s_x.reuse_at(A, s_x[kernel.B], kernel.B.axis[1], \"WB\")\nprint(hcl.lower(s_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the printed IR, you should be able to see a buffer ``WB`` with size\n(3, 3) being allocated. Moreover, in the ``produce WB`` scope, you should\nsee the update algorithm described above. Now let's test the function again.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hcl_Bx = hcl.asarray(np.zeros((4, 4)))\nf = hcl.build(s_x)\nf(hcl_A, hcl_F, hcl_Bx)\nprint(\"Output without WB:\")\nprint(hcl_B)\nprint(\"Output with WB:\")\nprint(hcl_Bx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see the same results with and without the window buffer.\n\n### Reuse at a Different Dimension: Linebuffer\nSimilarly, we can create a reuse buffer for two consecutive iterations of\n``y``. In this case, in each iteration of ``y``, we read an entire row from\ninput ``A``. Meanwhile, we update the reuse buffer by shifting up. Since it\nreads an entire line at a time, we call it a linebuffer ``LB``. The\noperation is illustrated in the figure below.\n\n.. figure:: ../../../tutorials/moving_x/Slide3.png\n   :scale: 60 %\n\nSimilar to the window buffer, we can introduce the linebuffer in HeteroCL by\nusing a single ``reuse_at`` API. We show the code below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s_y = hcl.create_schedule([A, F], kernel)\nLB = s_y.reuse_at(A, s_y[kernel.B], kernel.B.axis[0], \"LB\")\nprint(hcl.lower(s_y))\n\nhcl_By = hcl.asarray(np.zeros((4, 4)))\nf = hcl.build(s_y)\nf(hcl_A, hcl_F, hcl_By)\nprint(\"Output without LB:\")\nprint(hcl_B)\nprint(\"Output with LB:\")\nprint(hcl_By)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the difference between WB and LB is the we reuse at different\naxes. We can also see from the printed IR that the allocated size is larger,\nwhich is the same as illustrated in the figure above. In this case, we read\n6 pixels from the input for each iteration of ``y``, which means we read 1\npixel for each iteration of ``x`` **effectively**. Namely, this is not true\nin terms of hardware execution. Can we do even better?\n\n### Combine Window Buffer and Linebuffer\nWe do not need to restrict ourselves to reuse at a single dimension. Since\nwe have data reuse in both dimension, we can reuse both. In this case,\nwe generate both a linebuffer and a window buffer. Let's take a look at\nthe figure first.\n\n.. figure:: ../../../tutorials/moving_x/Slide4.png\n   :scale: 60 %\n\nWhat happens here is that, we first update the linebuffer (blue arrows),\nthen we update the window buffer (purple arrows). More precisely, for each\niteration of ``x``, we **read 1 pixel from input ``A``**. We simultaneously\nshift up the linebffer. After we update the linebuffer, we go on update the\nwindow buffer by reading pixels updated in the linebuffer. Then we shift the\nwindow buffer. To describe such behavior in HeteroCL is very easy. We only\nneed to apply ``reuse_at`` twice. We just need to specify the corresponding\nreuse tensors and the reuse axes. In this case, the linebuffer reuses the\npixels from the input ``A`` while the window buffer reuses from the\nlinebuffer. Following we show the code and its IR.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s_xy = hcl.create_schedule([A, F], kernel)\nLB = s_xy.reuse_at(A, s_xy[kernel.B], kernel.B.axis[0], \"LB\")\nWB = s_xy.reuse_at(LB, s_xy[kernel.B], kernel.B.axis[1], \"WB\")\nprint(hcl.lower(s_xy))\n\nhcl_Bxy = hcl.asarray(np.zeros((4, 4)))\nf = hcl.build(s_xy)\nf(hcl_A, hcl_F, hcl_Bxy)\nprint(\"Output without reuse buffers:\")\nprint(hcl_B)\nprint(\"Output with reuse buffers:\")\nprint(hcl_Bxy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the IR that the allocation sizes are indeed as expected.\n\n### Further Optimization\nTo further optimize the design, we need to think more carefully. For each\niteration of ``x``, there are three pixels in LB that are being read/write\nsimultaneously. Thus, to maximize the memory bandwidth, we need to partition\nLB in the row direction. For WB, all pixels are updated at the same time.\nTherefore, we need to partition the whole WB completely. Finally, we can\npipeline the whole design. Don't forget that we also need to partition the\nfilter ``F``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s_final = hcl.create_schedule([A, F], kernel)\nLB = s_final.reuse_at(A, s_final[kernel.B], kernel.B.axis[0], \"LB\")\nWB = s_final.reuse_at(LB, s_final[kernel.B], kernel.B.axis[1], \"WB\")\ns_final.partition(LB, dim=1)\ns_final.partition(WB)\ns_final.partition(F)\ns_final[kernel.B].pipeline(kernel.B.axis[1])\nprint(hcl.lower(s_final))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can generate the HLS code and see if the II is indeed 1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f = hcl.build(s_final, target=\"vhls\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following is a sample report from Vivado_HLS.\n\n.. code::\n\n   + Latency (clock cycles):\n       * Summary:\n       +-----+-----+-----+-----+---------+\n       |  Latency  |  Interval | Pipeline|\n       | min | max | min | max |   Type  |\n       +-----+-----+-----+-----+---------+\n       |   42|   42|   43|   43|   none  |\n       +-----+-----+-----+-----+---------+\n\n       + Detail:\n           * Instance:\n           N/A\n\n           * Loop:\n           +----------+-----+-----+----------+-----------+-----------+------+----------+\n           |          |  Latency  | Iteration|  Initiation Interval  | Trip |          |\n           | Loop Name| min | max |  Latency |  achieved |   target  | Count| Pipelined|\n           +----------+-----+-----+----------+-----------+-----------+------+----------+\n           |- Loop 1  |   40|   40|         6|          1|          1|    36|    yes   |\n           +----------+-----+-----+----------+-----------+-----------+------+----------+\n\n\n## Limitations\nFollowing we list the limitations of using reuse buffers in HeteroCL.\n\n1. We do not accept non-linear index patterns, e.g., ``y*y+c``, ``y*(y+c)``\n2. The stride is not one, e.g., ``2*y+c``\n3. There is no overlapped pixel between two consecutive iterations of the\n   specified axis, e.g., ``[x+r, y]`` and reuse ``y``\n\n## More Examples: 2D Image Blur\nHeteroCL is also able to infer reuse buffers for explicit reduction\noperations. Namely, instead of using ``hcl.sum``, we can expand the compute\npatterns. Following is an example of 2D blur.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hcl.init()\nA = hcl.placeholder((10, 10), \"A\")\n\n\ndef kernel_blur(A):\n    return hcl.compute(\n        (8, 8), lambda y, x: A[y, x] + A[y + 1, x + 1] + A[y + 2, x + 2], \"B\"\n    )\n\n\ns_blur = hcl.create_schedule(A, kernel_blur)\nB = kernel_blur.B\nRB_y = s_blur.reuse_at(A, s_blur[B], B.axis[0], \"RB_y\")\nRB_x = s_blur.reuse_at(RB_y, s_blur[B], B.axis[1], \"RB_x\")\nprint(hcl.lower(s_blur))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}